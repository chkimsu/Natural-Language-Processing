{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치단위로 padding하는 방법(dynamic padding) 1번째"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Too many arguments.\n",
      "\n",
      "usage: git clone [<options>] [--] <repo> [<dir>]\n",
      "\n",
      "    -v, --verbose         be more verbose\n",
      "    -q, --quiet           be more quiet\n",
      "    --progress            force progress reporting\n",
      "    -n, --no-checkout     don't create a checkout\n",
      "    --bare                create a bare repository\n",
      "    --mirror              create a mirror repository (implies bare)\n",
      "    -l, --local           to clone from a local repository\n",
      "    --no-hardlinks        don't use local hardlinks, always copy\n",
      "    -s, --shared          setup as shared repository\n",
      "    --recursive ...       alias of --recurse-submodules\n",
      "    --recurse-submodules[=<pathspec>]\n",
      "                          initialize submodules in the clone\n",
      "    -j, --jobs <n>        number of submodules cloned in parallel\n",
      "    --template <template-directory>\n",
      "                          directory from which templates will be used\n",
      "    --reference <repo>    reference repository\n",
      "    --reference-if-able <repo>\n",
      "                          reference repository\n",
      "    --dissociate          use --reference only while cloning\n",
      "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
      "    -b, --branch <branch>\n",
      "                          checkout <branch> instead of the remote's HEAD\n",
      "    -u, --upload-pack <path>\n",
      "                          path to git-upload-pack on the remote\n",
      "    --depth <depth>       create a shallow clone of that depth\n",
      "    --shallow-since <time>\n",
      "                          create a shallow clone since a specific time\n",
      "    --shallow-exclude <revision>\n",
      "                          deepen history of shallow clone, excluding rev\n",
      "    --single-branch       clone only one branch, HEAD or --branch\n",
      "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
      "    --shallow-submodules  any cloned submodules will be shallow\n",
      "    --separate-git-dir <gitdir>\n",
      "                          separate git dir from working tree\n",
      "    -c, --config <key=value>\n",
      "                          set config inside the new repository\n",
      "    --server-option <server-specific>\n",
      "                          option to transmit\n",
      "    -4, --ipv4            use IPv4 addresses only\n",
      "    -6, --ipv6            use IPv6 addresses only\n",
      "    --filter <args>       object filtering\n",
      "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/e9t/nsmc.git  ### naver 뉴스댓글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')\n",
    "\n",
    "train = train[~pd.isnull(train['document'])]\n",
    "train_text = train['document'].values\n",
    "labels = train.label.tolist()\n",
    "train_text = train_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "## input to ids 만 하는것\n",
    "\n",
    "full_input_ids = []\n",
    "\n",
    "# For each training example...\n",
    "for text in train_text:\n",
    "\n",
    "    # Tokenize the sentence.\n",
    "    input_ids = tokenizer.encode(text=text,           # Movie review text\n",
    "                                 add_special_tokens=True, # Do add specials.\n",
    "                                #  max_length=max_len,  # Do truncate to `max_len`\n",
    "                                #  truncation=True,     # Do truncate!\n",
    "                                 padding=False)       # Don't pad!\n",
    "                                 \n",
    "    # Add the tokenized result to our list.\n",
    "    full_input_ids.append(input_ids)\n",
    "    \n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 한 단계 더 나아가서 input 길이를 sorting해서 작은것끼리 배치로 묶는 방법 사용. \n",
    "### 이는 블로그를 참고했고, 작은 길이는 작은길이끼리 배치로 묶는게 컴퓨터 리소스 적게 먹는다는 아이디어. \n",
    "\n",
    "train_samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training batches of size 16\n",
      "  Selected 0 batches.\n",
      "  Selected 500 batches.\n",
      "  Selected 1,000 batches.\n",
      "  Selected 1,500 batches.\n",
      "  Selected 2,000 batches.\n",
      "  Selected 2,500 batches.\n",
      "  Selected 3,000 batches.\n",
      "  Selected 3,500 batches.\n",
      "  Selected 4,000 batches.\n",
      "  Selected 4,500 batches.\n",
      "  Selected 5,000 batches.\n",
      "  Selected 5,500 batches.\n",
      "  Selected 6,000 batches.\n",
      "  Selected 6,500 batches.\n",
      "  Selected 7,000 batches.\n",
      "  Selected 7,500 batches.\n",
      "  Selected 8,000 batches.\n",
      "  Selected 8,500 batches.\n",
      "  Selected 9,000 batches.\n",
      "\n",
      "  DONE - 9,375 batches.\n"
     ]
    }
   ],
   "source": [
    "### 배치 자체를 만드는 코드\n",
    "\n",
    "import random\n",
    "\n",
    "batch_size = 16\n",
    "# List of batches that we'll construct.\n",
    "batch_ordered_sentences = []\n",
    "batch_ordered_labels = []\n",
    "\n",
    "print('Creating training batches of size {:}'.format(batch_size))\n",
    "\n",
    "# Loop over all of the input samples...    \n",
    "while len(train_samples) > 0:\n",
    "    \n",
    "    # Report progress.\n",
    "    if ((len(batch_ordered_sentences) % 500) == 0):\n",
    "        print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # `to_take` is our actual batch size. It will be `batch_size` until \n",
    "    # we get to the last batch, which may be smaller. \n",
    "    to_take = min(batch_size, len(train_samples))\n",
    "\n",
    "    # Pick a random index in the list of remaining samples to start\n",
    "    # our batch at.\n",
    "    select = random.randint(0, len(train_samples) - to_take)\n",
    "\n",
    "    # Select a contiguous batch of samples starting at `select`.\n",
    "    batch = train_samples[select:(select + to_take)]\n",
    "\n",
    "    # Each sample is a tuple--split them apart to create a separate list of \n",
    "    # sequences and a list of labels for this batch.\n",
    "    batch_ordered_sentences.append([s[0] for s in batch])\n",
    "    batch_ordered_labels.append([s[1] for s in batch])\n",
    "\n",
    "    # Remove these samples from the list.\n",
    "    del train_samples[select:select + to_take]\n",
    "\n",
    "print('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 만든 배치를 이용해서 배치단위로 padding하는 것. \n",
    "### 이렇게 하기 귀찮아서..그냥 고정길이로 다들 하는건가...\n",
    "### 배치단위로 이렇게 PADDING 하기 위해서 FOR문을 사용하는 것 여긴..\n",
    " \n",
    "import torch\n",
    "\n",
    "py_inputs = []\n",
    "py_attn_masks = []\n",
    "py_labels = []\n",
    "\n",
    "# For each batch...\n",
    "for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
    "\n",
    "    # New version of the batch, this time with padded sequences and now with\n",
    "    # attention masks defined.\n",
    "    batch_padded_inputs = []\n",
    "    batch_attn_masks = []\n",
    "    \n",
    "    # First, find the longest sample in the batch. \n",
    "    # Note that the sequences do currently include the special tokens!\n",
    "    max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "    #print('Max size:', max_size)\n",
    "\n",
    "    # For each input in this batch...\n",
    "    for sen in batch_inputs:\n",
    "        \n",
    "        # How many pad tokens do we need to add?\n",
    "        num_pads = max_size - len(sen)\n",
    "\n",
    "        # Add `num_pads` padding tokens to the end of the sequence.\n",
    "        padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "        # Define the attention mask--it's just a `1` for every real token\n",
    "        # and a `0` for every padding token.\n",
    "        attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "        # Add the padded results to the batch.\n",
    "        batch_padded_inputs.append(padded_input)\n",
    "        batch_attn_masks.append(attn_mask)\n",
    "\n",
    "    # Our batch has been padded, so we need to save this updated batch.\n",
    "    # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
    "    py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "    py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "    py_labels.append(torch.tensor(batch_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50])\n",
      "torch.Size([16, 205])\n",
      "torch.Size([16, 48])\n"
     ]
    }
   ],
   "source": [
    "print(py_inputs[0].shape)\n",
    "print(py_inputs[1].shape)\n",
    "print(py_inputs[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-15.2023, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### 이렇게 배치간 길이가 달라도 학습이 가능하다는것을 증명!!!\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "logits = model(py_inputs[0], attention_mask=py_attn_masks[0])\n",
    "loss = torch.nn.functional.nll_loss(logits['logits'], py_labels[0])\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "logits = model(py_inputs[1], attention_mask=py_attn_masks[1])\n",
    "loss = torch.nn.functional.nll_loss(logits['logits'], py_labels[1])\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.1566, 0.2231],\n",
       "        [0.1555, 0.2490],\n",
       "        [0.1566, 0.2217],\n",
       "        [0.1744, 0.2401],\n",
       "        [0.1794, 0.2326],\n",
       "        [0.1527, 0.2560],\n",
       "        [0.1704, 0.2244],\n",
       "        [0.1468, 0.2280],\n",
       "        [0.1678, 0.2448],\n",
       "        [0.1381, 0.2351],\n",
       "        [0.1279, 0.2492],\n",
       "        [0.1617, 0.2286],\n",
       "        [0.1621, 0.2316],\n",
       "        [0.1741, 0.2248],\n",
       "        [0.1664, 0.2176],\n",
       "        [0.1614, 0.2449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 이런식으로 내가 직접 input_ids, attention_mask dictionary 형태로 만들어서 집어넣는것도 가능하다. \n",
    "model(**{'input_ids' : py_inputs[0], 'attention_mask' : py_attn_masks[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,   100,  1464, 30019, 30024, 30000, 30014, 30025, 29994, 30010,\n",
       "          30020, 29999, 30019,  1469, 30017, 30003, 30017, 29994, 30008, 30000,\n",
       "          30019, 30000, 30019,   100,   100,  1463, 30010, 30021, 29991, 30019,\n",
       "          29999, 30012,   100,  1455, 30010, 30022, 29995, 30006, 30022, 29993,\n",
       "          30011,   100,  1460, 30011, 30024, 29992, 30019, 29993, 30006,   102],\n",
       "         [  101,  1456, 30006,  1464, 30006, 29997, 30019, 30021, 29999, 30017,\n",
       "          30022,  1457, 30011, 30022, 29999, 30019, 30002, 30010, 29996, 30011,\n",
       "          29991, 30009, 30005, 30006, 29991, 30011, 29996, 30006, 30021, 29997,\n",
       "          30008, 30025, 30005, 30006, 29991, 30009,  1459, 30006, 30021, 29993,\n",
       "          30017, 29992, 30017, 30021,  1463, 30010, 30025, 30005, 30012,   102],\n",
       "         [  101,  1005,  1464, 30014, 29996, 30014,  1005,  1458, 30006, 29992,\n",
       "          30017, 30021,   100,  1457, 30007, 30005, 30007,  1457, 30006, 29997,\n",
       "          30019, 30005, 30006, 30021, 29996, 30008, 30021,  1461, 30007, 30025,\n",
       "          29991, 30006, 30020, 30005, 30006, 29991, 30009,   100,   100,  1459,\n",
       "          30006, 30000, 30019, 29995, 30006, 30020, 29999, 30019,   100,   102],\n",
       "         [  101,  1459, 30009, 30022, 29991, 30019, 30024, 29997, 30017, 30021,\n",
       "          29999, 30017, 30022,   100,  1463, 30010, 30025, 30005, 30012,  1012,\n",
       "           1455, 30008, 30001, 30019, 30022, 30000, 30019, 29995, 30006, 30021,\n",
       "            100,  1460, 30008, 29993, 30019,  1469, 30010, 30025, 29997, 30006,\n",
       "          29995, 30014, 30022, 29999, 30018,  1456, 30016,   100,  1012,   102],\n",
       "         [  101,  1461, 30006, 30000, 30011, 30020,   100,  1460, 30006, 30021,\n",
       "          30000, 30008, 30021,  1012,  1455, 30017, 30000, 30008,   100,   100,\n",
       "           1464, 30011, 29999, 30006, 30020, 30005, 30006, 30021,  1455, 30010,\n",
       "          30022, 29994, 30011, 30021, 29999, 30017, 30022,  1456, 30007, 29994,\n",
       "          30019, 29991, 30011,  1459, 30006, 30021, 29993, 30006,  1012,   102],\n",
       "         [  101,  1463, 30008, 29992, 30017,  1464, 30008, 30025, 29997, 30019,\n",
       "          30021, 29999, 30019, 29997, 30006, 30025, 30000, 30006, 29999, 30018,\n",
       "           1463, 30019, 30022, 29997, 30007, 30025, 29991, 30012,  1455, 30006,\n",
       "          30000, 30011, 30020, 29997, 30006, 29994, 30017, 30022,  1457, 30006,\n",
       "          29994, 30014, 30021,  1464, 30006, 30020, 30004, 30014, 30023,   102],\n",
       "         [  101,  1463, 30019,  1463, 30010, 30025, 30005, 30012, 29994, 30017,\n",
       "          30022,  1460, 30011, 30021, 29993, 30006, 29992, 30017, 30021,   100,\n",
       "           1010,  1463, 30019, 30021, 29997, 30007, 30025, 29999, 30018,  1463,\n",
       "          30011, 30000, 30008, 30023, 29999, 30017, 30022,  1456, 30006, 30023,\n",
       "          29991, 30019, 30021, 29993, 30006, 29992, 30017, 30021,   100,   102],\n",
       "         [  101,  1463, 30006,  1010,  1464, 30008, 30025, 29995, 30006, 30022,\n",
       "           1459, 30014, 30025, 30002, 30017, 30022, 30005, 30006, 29995, 30010,\n",
       "          30021, 29997, 30008,   100,   100,  1010,   100,  1456, 30007, 29991,\n",
       "          30006,   100,  1461, 30017, 30003, 30006, 29999, 30019, 30022, 29999,\n",
       "          30018,  1463, 30010, 30025, 30005, 30012,   999,   999,   100,   102],\n",
       "         [  101,   100,  1464, 30014, 30022, 29991, 30008, 29994, 30019, 29999,\n",
       "          30009,  1455, 30010, 30022, 30002, 30011,   100,   100,  1005,  1461,\n",
       "          30017, 30003, 30006, 29999, 30019, 30022,  1005,  1012,  1012,  1465,\n",
       "          30006, 30023,  1459, 30007, 29994, 30010, 30020, 30000, 30008, 30020,\n",
       "          29999, 30019, 30021,  1463, 30010, 30025, 30005, 30012,  1012,   102],\n",
       "         [  101,  1456, 30006, 30021,  1463, 30006, 30000, 30019, 30020,  1455,\n",
       "          30017, 29992, 30010, 29991, 30006,  1455, 30010, 30022, 30005, 30011,\n",
       "          30021, 30005, 30006, 30000, 30019,   100,  1010,  1457, 30011, 30022,\n",
       "          29999, 30006, 29999, 30011, 29991, 30019, 29994, 30017, 30022,  1460,\n",
       "          30006, 29994, 30006, 30021, 29993, 30006,  1012,  1012,  1012,   102],\n",
       "         [  101,   100,  1463, 30010, 30000, 30008, 30021, 29997, 30006,  1457,\n",
       "          30014, 29995, 30010, 30025, 29995, 30006, 30022, 29991, 30011, 29992,\n",
       "          30017, 30021,  1463, 30010, 30025, 30005, 30012, 29991, 30006,  1469,\n",
       "          30006, 30004, 30014, 30023, 29992, 30006, 29999, 30011, 29992, 30017,\n",
       "          30021,  1461, 30014, 30000, 30014, 30021,  1012,  1012,  1012,   102],\n",
       "         [  101,   100,  1462, 30017, 29994, 30009, 29991, 30019, 29994, 30017,\n",
       "          30022,  1457, 30011, 29993, 30009, 30001, 30009,  1459, 30015, 30021,\n",
       "          29997, 30007, 30025, 29991, 30006, 30020, 29999, 30017, 29994, 30011,\n",
       "            100,  1012,  1012,  1021, 29991, 30012, 30025, 29991, 30014, 29999,\n",
       "          30009,   100,  1469, 30015, 30022, 29998, 30019, 30021,   100,   102],\n",
       "         [  101,  1455, 30017, 30020, 30000, 30006, 30025, 30004, 30006, 30021,\n",
       "          29999, 30019, 30021, 29991, 30009,  1464, 30011, 30023,   100,  1010,\n",
       "           1467, 30019, 29996, 30019,  1463, 30009, 30004, 30019, 29997, 30011,\n",
       "          29993, 30017,  1464, 30014, 30025,  1469, 30006, 29992, 30006, 29994,\n",
       "          30011,  1463, 30010, 29991, 30019, 29995, 30010, 30021,   100,   102],\n",
       "         [  101,   100,   100,  1469, 30019, 30023, 29993, 30017, 30022, 29999,\n",
       "          30008, 29991, 30006, 30021,  1461, 30006, 29995, 30014, 29994, 30006,\n",
       "          29999, 30019, 29991, 30006,  1463, 30006, 29992, 30019, 29999, 30008,\n",
       "          29997, 30008,   100,  1012,  1455, 30017, 29994, 30019, 29991, 30011,\n",
       "           1469, 30019, 29994, 30011, 29999, 30016, 30002, 30019,  1012,   102],\n",
       "         [  101,  1468, 30010, 30025, 29994, 30011, 30021, 29991, 30006,  1464,\n",
       "          30008, 30023, 29997, 30014, 29991, 30006,  1020, 30000, 30008, 30023,\n",
       "          29993, 30007, 29999, 30019, 30021,  1463, 30019, 29999, 30016, 29994,\n",
       "          30017, 30022,  1463, 30006, 30022,  1461, 30014,   100,  1012,   100,\n",
       "            100,  1463, 30010, 30025, 30005, 30012,  1012,   100,  1012,   102],\n",
       "         [  101,  1463, 30010, 30025, 30005, 30012, 29996, 30011, 29991, 30014,\n",
       "            100,  1464, 30008, 30023,   100,  1469, 30006, 29992, 30006,  1457,\n",
       "          30006, 29997, 30019, 30021,  1461, 30008, 29991, 30017, 30020, 29991,\n",
       "          30006, 30023, 29993, 30011, 30020,  1463, 30010, 30025, 30005, 30012,\n",
       "          29992, 30017, 30021,  1463, 30006, 30021, 29996, 30012,  1012,   102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 더불어 이런식으로 여러개 tensor를 한번에 넣을때는 이렇게 넣으면되는것. \n",
    "py_inputs[0], py_attn_masks[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 149,995 training samples...\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 2,400 samples.\n",
      "  Tokenized 4,800 samples.\n",
      "  Tokenized 7,200 samples.\n",
      "  Tokenized 9,600 samples.\n",
      "  Tokenized 12,000 samples.\n",
      "  Tokenized 14,400 samples.\n",
      "  Tokenized 16,800 samples.\n",
      "  Tokenized 19,200 samples.\n",
      "  Tokenized 21,600 samples.\n",
      "  Tokenized 24,000 samples.\n",
      "  Tokenized 26,400 samples.\n",
      "  Tokenized 28,800 samples.\n",
      "  Tokenized 31,200 samples.\n",
      "  Tokenized 33,600 samples.\n",
      "  Tokenized 36,000 samples.\n",
      "  Tokenized 38,400 samples.\n",
      "  Tokenized 40,800 samples.\n",
      "  Tokenized 43,200 samples.\n",
      "  Tokenized 45,600 samples.\n",
      "  Tokenized 48,000 samples.\n",
      "  Tokenized 50,400 samples.\n",
      "  Tokenized 52,800 samples.\n",
      "  Tokenized 55,200 samples.\n",
      "  Tokenized 57,600 samples.\n",
      "  Tokenized 60,000 samples.\n",
      "  Tokenized 62,400 samples.\n",
      "  Tokenized 64,800 samples.\n",
      "  Tokenized 67,200 samples.\n",
      "  Tokenized 69,600 samples.\n",
      "  Tokenized 72,000 samples.\n",
      "  Tokenized 74,400 samples.\n",
      "  Tokenized 76,800 samples.\n",
      "  Tokenized 79,200 samples.\n",
      "  Tokenized 81,600 samples.\n",
      "  Tokenized 84,000 samples.\n",
      "  Tokenized 86,400 samples.\n",
      "  Tokenized 88,800 samples.\n",
      "  Tokenized 91,200 samples.\n",
      "  Tokenized 93,600 samples.\n",
      "  Tokenized 96,000 samples.\n",
      "  Tokenized 98,400 samples.\n",
      "  Tokenized 100,800 samples.\n",
      "  Tokenized 103,200 samples.\n",
      "  Tokenized 105,600 samples.\n",
      "  Tokenized 108,000 samples.\n",
      "  Tokenized 110,400 samples.\n",
      "  Tokenized 112,800 samples.\n",
      "  Tokenized 115,200 samples.\n",
      "  Tokenized 117,600 samples.\n",
      "  Tokenized 120,000 samples.\n",
      "  Tokenized 122,400 samples.\n",
      "  Tokenized 124,800 samples.\n",
      "  Tokenized 127,200 samples.\n",
      "  Tokenized 129,600 samples.\n",
      "  Tokenized 132,000 samples.\n",
      "  Tokenized 134,400 samples.\n",
      "  Tokenized 136,800 samples.\n",
      "  Tokenized 139,200 samples.\n",
      "  Tokenized 141,600 samples.\n",
      "  Tokenized 144,000 samples.\n",
      "  Tokenized 146,400 samples.\n",
      "  Tokenized 148,800 samples.\n"
     ]
    }
   ],
   "source": [
    "### 이건 참고로, 모든 sequence의 길이를 통일하는 방법. 이렇게 하면 모든 batch가 동일한 길이로 padding되는 듯하다. \n",
    "\n",
    "use_fixed_padding = True\n",
    "\n",
    "if use_fixed_padding:\n",
    "\n",
    "    # Specify batch_size and truncation length.    \n",
    "    batch_size = 16\n",
    "    max_len = 400   \n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} training samples...'.format(len(train_text)))\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    batches_input_ids = []\n",
    "    batches_attention_masks = []\n",
    "    batches_labels = []\n",
    "\n",
    "    update_interval = batch_size * 150 \n",
    "\n",
    "    # For every sentence...\n",
    "    for i in range(0, len(train_text), batch_size):\n",
    "\n",
    "        # Report progress.\n",
    "        if ((i % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(i))\n",
    "\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.batch_encode_plus(\n",
    "                            train_text[i:i+batch_size], # Batch of sentences to encode.\n",
    "                            add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 400,           # Pad & truncate all sentences.\n",
    "                            padding = 'max_length',     # Pad all to the `max_length` parameter.\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        batches_input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        batches_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Add the labels for the batch\n",
    "        batches_labels.append(torch.tensor(labels[i:i+batch_size]))\n",
    "    \n",
    "    # Rename the final variable to match the rest of the code in this Notebook.\n",
    "    py_inputs = batches_input_ids\n",
    "    py_attn_masks = batches_attention_masks\n",
    "    py_labels = batches_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 400])\n",
      "torch.Size([16, 400])\n",
      "torch.Size([16, 400])\n"
     ]
    }
   ],
   "source": [
    "print(py_inputs[0].shape)\n",
    "print(py_inputs[1].shape)\n",
    "print(py_inputs[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path='distilbert-base-uncased')\n",
    "\n",
    "## distilbert 자체는 학습하지 않도록. \n",
    "\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0221,  0.0464, -0.0018,  ..., -0.0121,  0.0260,  0.0291],\n",
      "        [ 0.0165,  0.0118, -0.0105,  ...,  0.0240, -0.0199, -0.0137]])\n",
      "tensor([[-0.0166, -0.0666, -0.0163,  ..., -0.0200, -0.0514, -0.0264],\n",
      "        [-0.0132, -0.0673, -0.0161,  ..., -0.0227, -0.0554, -0.0260],\n",
      "        [-0.0176, -0.0709, -0.0144,  ..., -0.0246, -0.0596, -0.0232],\n",
      "        ...,\n",
      "        [-0.0231, -0.0588, -0.0105,  ..., -0.0195, -0.0262, -0.0212],\n",
      "        [-0.0490, -0.0561, -0.0047,  ..., -0.0107, -0.0180, -0.0219],\n",
      "        [-0.0065, -0.0915, -0.0025,  ..., -0.0151, -0.0504,  0.0460]])\n",
      "tensor([0.8425, 0.8005, 0.7313, 0.7595, 0.7978, 0.7835, 0.8177, 0.8189, 0.8398,\n",
      "        0.7094, 0.6675, 0.6861, 0.8101, 0.6887, 0.8038, 0.7560, 0.7175, 0.7307,\n",
      "        0.8399, 0.7757, 0.8152, 0.7324, 0.8066, 0.7566, 0.7473, 0.8094, 0.6373,\n",
      "        0.8413, 0.7010, 0.7445, 0.8248, 0.7532, 0.8007, 0.8824, 0.8201, 0.7667,\n",
      "        0.8325, 0.7654, 0.8201, 0.7824, 0.8443, 0.8523, 0.7780, 0.7448, 0.7763,\n",
      "        0.7377, 0.6849, 0.8719, 0.8288, 0.8497, 0.8356, 0.7474, 0.8634, 0.2268,\n",
      "        0.7491, 0.7083, 0.7760, 0.7117, 0.7918, 0.6601, 0.7908, 0.7184, 0.2888,\n",
      "        0.7677, 0.8005, 0.7626, 0.7850, 0.8738, 0.7818, 0.7709, 0.8526, 0.8576,\n",
      "        0.8500, 0.6365, 0.8522, 0.6601, 0.7488, 0.2975, 0.6800, 0.7334, 0.7895,\n",
      "        0.8532, 0.8371, 0.8002, 0.6407, 0.7231, 0.7409, 0.7807, 0.8158, 0.8030,\n",
      "        0.7470, 0.7510, 0.5425, 0.7456, 0.7747, 0.6928, 0.2944, 0.6655, 0.8017,\n",
      "        0.8320, 0.8543, 0.8706, 0.7490, 0.7002, 0.6721, 0.2078, 0.8188, 0.7823,\n",
      "        0.7622, 0.6928, 0.8551, 0.7606, 0.7567, 0.8218, 0.8284, 0.7424, 0.8015,\n",
      "        0.8228, 0.7928, 0.7859, 0.7522, 0.0859, 0.7630, 0.7647, 0.5456, 0.7390,\n",
      "        0.7445, 0.5827, 0.7947, 0.7373, 0.8003, 0.7733, 0.2957, 0.7636, 0.8016,\n",
      "        0.8388, 0.7932, 0.7061, 0.8098, 0.2505, 0.8461, 0.8098, 0.8361, 0.8259,\n",
      "        0.8411, 0.1325, 0.8156, 0.7403, 0.7458, 0.7288, 0.6791, 0.7915, 0.6145,\n",
      "        0.7914, 0.8779, 0.7721, 0.8181, 0.7276, 0.8194, 0.1516, 0.8336, 0.8378,\n",
      "        0.7347, 0.9317, 0.8010, 0.8412, 0.7457, 0.7246, 0.2098, 0.8024, 0.8727,\n",
      "        0.7811, 0.6902, 0.8584, 0.8504, 0.1366, 0.7179, 0.7260, 0.7524, 0.7520,\n",
      "        0.7598, 0.8569, 0.7457, 0.8377, 0.8563, 0.8060, 0.8476, 0.7624, 0.8385,\n",
      "        0.7280, 0.7858, 0.8011, 0.7312, 0.7557, 0.7977, 0.7969, 0.8166, 0.8021,\n",
      "        0.6976, 0.6706, 0.7621, 0.8224, 0.1831, 0.7642, 0.7640, 0.7003, 0.7912,\n",
      "        0.7399, 0.8708, 0.7408, 0.7345, 0.8529, 0.7039, 0.7920, 0.7502, 0.1899,\n",
      "        0.1041, 0.8139, 0.3663, 0.7292, 0.8421, 0.7799, 0.8777, 0.7520, 0.7325,\n",
      "        0.2458, 0.7220, 0.6516, 0.8404, 0.8132, 0.7653, 0.7561, 0.7441, 0.8359,\n",
      "        0.8345, 0.7686, 0.7412, 0.6681, 0.8497, 0.6728, 0.7272, 0.8016, 0.8316,\n",
      "        0.7760, 0.7243, 0.2786, 0.7240, 0.7294, 0.7202, 0.3829, 0.7830, 0.7707,\n",
      "        0.7600, 0.7652, 0.8157, 0.7276, 0.7202, 0.7549, 0.7212, 0.7563, 0.8582,\n",
      "        0.7640, 0.8510, 0.8005, 0.6367, 0.7305, 0.8009, 0.8029, 0.7939, 0.8288,\n",
      "        0.7583, 0.7710, 0.7578, 0.7684, 0.8767, 0.8334, 0.7093, 0.7717, 0.7601,\n",
      "        0.8194, 0.7206, 0.8028, 0.8436, 0.2132, 0.8029, 0.7320, 0.7605, 0.8205,\n",
      "        0.7186, 0.8638, 0.7442, 0.3013, 0.7588, 0.7771, 0.7539, 0.6480, 0.2554,\n",
      "        0.8160, 0.7496, 0.7694, 0.7949, 0.8096, 0.8174, 0.8084, 0.6491, 0.8146,\n",
      "        0.7221, 0.7849, 0.5801, 0.7922, 0.7394, 0.2555, 0.7655, 0.8112, 0.8402,\n",
      "        0.7376, 0.8210, 0.8095, 0.7888, 0.3207, 0.7938, 0.7490, 0.7549, 0.7898,\n",
      "        0.7921, 0.8186, 0.7336, 0.8276, 0.7920, 0.7364, 0.7007, 0.3114, 0.7771,\n",
      "        0.8236, 0.7715, 0.7361, 0.7277, 0.8477, 0.8211, 0.8228, 0.6200, 0.8466,\n",
      "        0.7333, 0.7984, 0.7368, 0.2651, 0.8359, 0.7306, 0.8008, 0.8766, 0.8250,\n",
      "        0.8694, 0.8172, 0.1956, 0.8328, 0.8311, 0.7665, 0.7613, 0.7750, 0.7524,\n",
      "        0.8148, 0.7356, 0.7233, 0.7276, 0.8007, 0.7779, 0.7451, 0.7815, 0.8293,\n",
      "        0.6922, 0.7830, 0.7247, 0.8446, 0.7455, 0.7880, 0.9051, 0.7991, 0.7721,\n",
      "        0.7235, 0.8439, 0.7791, 0.3232, 0.8118, 0.8268, 0.7821, 0.8112, 0.7882,\n",
      "        0.8316, 0.8050, 0.7507, 0.6424, 0.7401, 0.7723, 0.7365, 0.8553, 0.8052,\n",
      "        0.7473, 0.7266, 0.7518, 0.8425, 0.7490, 0.4735, 0.7326, 0.7749, 0.6550,\n",
      "        0.7598, 0.8196, 0.7962, 0.7603, 0.8455, 0.8304, 0.7779, 0.8690, 0.7906,\n",
      "        0.7657, 0.7232, 0.7646, 0.7920, 0.7906, 0.2222, 0.7321, 0.6409, 0.7818,\n",
      "        0.7223, 0.7561, 0.7349, 0.7863, 0.8207, 0.8210, 0.6678, 0.7929, 0.6849,\n",
      "        0.8224, 0.6908, 0.7975, 0.8961, 0.7682, 0.6965, 0.7924, 0.7515, 0.7973,\n",
      "        0.7852, 0.7308, 0.2778, 0.8134, 0.6690, 0.8699, 0.7585, 0.7961, 0.7383,\n",
      "        0.7595, 0.8590, 0.7970, 0.7991, 0.8468, 0.8895, 0.7789, 0.7550, 0.7185,\n",
      "        0.7894, 0.7549, 0.2577, 0.8245, 0.8336, 0.7608, 0.7221, 0.7086, 0.6825,\n",
      "        0.7257, 0.7313, 0.7841, 0.8112, 0.7718, 0.7955, 0.7077, 0.7788, 0.7295,\n",
      "        0.7965, 0.7551, 0.7385, 0.8320, 0.7559, 0.2502, 0.7917, 0.7681, 0.7621,\n",
      "        0.7663, 0.7629, 0.7130, 0.7854, 0.8311, 0.7599, 0.7456, 0.8403, 0.8088,\n",
      "        0.7871, 0.2660, 0.7285, 0.8482, 0.7103, 0.7069, 0.7980, 0.7394, 0.6892,\n",
      "        0.6851, 0.7844, 0.7499, 0.7444, 0.8040, 0.8043, 0.5894, 0.6714, 0.7921,\n",
      "        0.7899, 0.7573, 0.7816, 0.7674, 0.2630, 0.8033, 0.7589, 0.8377, 0.6753,\n",
      "        0.8244, 0.7487, 0.7649, 0.8117, 0.7807, 0.8462, 0.8042, 0.1437, 0.6919,\n",
      "        0.7313, 0.8561, 0.8156, 0.8102, 0.7798, 0.8044, 0.7746, 0.7044, 0.7959,\n",
      "        0.7243, 0.1237, 0.8263, 0.6973, 0.7964, 0.2443, 0.7769, 0.7487, 0.7347,\n",
      "        0.7825, 0.8071, 0.6846, 0.8261, 0.8203, 0.8496, 0.8232, 0.7217, 0.6763,\n",
      "        0.8029, 0.8302, 0.8343, 0.7410, 0.7378, 0.8219, 0.7120, 0.7794, 0.7909,\n",
      "        0.7970, 0.7891, 0.8326, 0.7259, 0.8081, 0.7738, 0.5392, 0.8033, 0.7986,\n",
      "        0.8253, 0.2257, 0.8043, 0.8157, 0.7496, 0.8365, 0.8051, 0.7322, 0.8151,\n",
      "        0.7499, 0.8111, 0.8003, 0.6811, 0.8384, 0.8045, 0.8132, 0.8746, 0.7578,\n",
      "        0.6777, 0.6958, 0.7867, 0.7432, 0.8259, 0.7645, 0.7766, 0.8594, 0.8198,\n",
      "        0.7210, 0.7654, 0.7148, 0.8812, 0.7008, 0.8131, 0.7299, 0.8183, 0.6835,\n",
      "        0.8086, 0.6624, 0.2636, 0.7853, 0.7371, 0.9013, 0.7996, 0.7393, 0.7189,\n",
      "        0.8034, 0.8459, 0.7760, 0.7821, 0.7549, 0.7916, 0.7639, 0.7268, 0.8172,\n",
      "        0.7345, 0.7278, 0.7416, 0.7774, 0.7924, 0.7941, 0.8186, 0.8396, 0.7104,\n",
      "        0.8351, 0.7564, 0.7192, 0.7926, 0.7035, 0.8210, 0.7127, 0.7601, 0.7884,\n",
      "        0.7593, 0.7395, 0.8733, 0.8337, 0.7219, 0.7469, 0.1225, 0.7224, 0.6651,\n",
      "        0.7723, 0.8223, 0.7785, 0.8131, 0.8559, 0.8188, 0.7936, 0.8501, 0.7702,\n",
      "        0.8069, 0.7817, 0.7879, 0.7429, 0.2991, 0.7761, 0.7191, 0.7038, 0.8204,\n",
      "        0.7845, 0.7679, 0.7749, 0.8638, 0.7579, 0.8167, 0.8554, 0.7357, 0.8161,\n",
      "        0.9011, 0.7792, 0.7295, 0.8480, 0.8364, 0.8145, 0.7901, 0.7947, 0.7210,\n",
      "        0.8773, 0.7544, 0.7750, 0.7122, 0.6910, 0.7477, 0.2338, 0.8290, 0.7895,\n",
      "        0.7559, 0.7395, 0.8262, 0.6973, 0.6339, 0.7452, 0.8102, 0.7933, 0.6548,\n",
      "        0.7687, 0.6668, 0.7920, 0.7347, 0.8432, 0.7073, 0.7292, 0.8372, 0.7189,\n",
      "        0.7149, 0.8043, 0.8596, 0.8502, 0.7932, 0.8357, 0.7641, 0.6555, 0.7721,\n",
      "        0.7829, 0.7854, 0.8262, 0.8612, 0.6990, 0.7270, 0.7458, 0.8460, 0.6910,\n",
      "        0.8758, 0.6756, 0.7351, 0.8205, 0.8621, 0.7800, 0.7316, 0.7755, 0.7001,\n",
      "        0.7393, 0.7687, 0.7912, 0.8803, 0.8465, 0.7670, 0.7903, 0.8335, 0.7099,\n",
      "        0.7163, 0.8329, 0.7931, 0.6378, 0.8097, 0.7553, 0.7360, 0.8020, 0.8034,\n",
      "        0.6588, 0.7901, 0.9008])\n"
     ]
    }
   ],
   "source": [
    "### 학습 전 가중치 확인. \n",
    "print(model.classifier.state_dict()['weight'])\n",
    "print(model.distilbert.state_dict()['embeddings.word_embeddings.weight'])\n",
    "print(model.distilbert.state_dict()['embeddings.LayerNorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # This is the value Michael used.\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. I chose to train for 1 simply because the training\n",
    "# time is long. More epochs may improve the model's accuracy.\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# Note that it's the number of *batches*, not *samples*!\n",
    "total_steps = len(py_inputs) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training on 9,375 batches...\n",
      "0 10\n",
      "1 10\n",
      "2 10\n",
      "3 10\n",
      "4 10\n",
      "5 10\n",
      "6 10\n",
      "7 10\n",
      "8 10\n",
      "9 10\n"
     ]
    }
   ],
   "source": [
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "     \n",
    "    print('Training on {:,} batches...'.format(len(py_inputs)))\n",
    "  \n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step in range(0, 10): ## len(py_inputs) -- 일부러 10개로만 해보자. \n",
    "      \n",
    "        print(step, 10)\n",
    "        # Copy the current training batch to the GPU using the `to` method.\n",
    "        b_input_ids = py_inputs[step]\n",
    "        b_input_mask = py_attn_masks[step]\n",
    "        b_labels = py_labels[step]\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass.\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The call returns the loss (because we provided labels) and the \n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)['loss'], model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)['logits']\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(py_inputs)            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0222,  0.0461, -0.0020,  ..., -0.0120,  0.0259,  0.0290],\n",
      "        [ 0.0167,  0.0121, -0.0104,  ...,  0.0240, -0.0197, -0.0135]])\n",
      "tensor([[-0.0166, -0.0666, -0.0163,  ..., -0.0200, -0.0514, -0.0264],\n",
      "        [-0.0132, -0.0673, -0.0161,  ..., -0.0227, -0.0554, -0.0260],\n",
      "        [-0.0176, -0.0709, -0.0144,  ..., -0.0246, -0.0596, -0.0232],\n",
      "        ...,\n",
      "        [-0.0231, -0.0588, -0.0105,  ..., -0.0195, -0.0262, -0.0212],\n",
      "        [-0.0490, -0.0561, -0.0047,  ..., -0.0107, -0.0180, -0.0219],\n",
      "        [-0.0065, -0.0915, -0.0025,  ..., -0.0151, -0.0504,  0.0460]])\n",
      "tensor([0.8425, 0.8005, 0.7313, 0.7595, 0.7978, 0.7835, 0.8177, 0.8189, 0.8398,\n",
      "        0.7094, 0.6675, 0.6861, 0.8101, 0.6887, 0.8038, 0.7560, 0.7175, 0.7307,\n",
      "        0.8399, 0.7757, 0.8152, 0.7324, 0.8066, 0.7566, 0.7473, 0.8094, 0.6373,\n",
      "        0.8413, 0.7010, 0.7445, 0.8248, 0.7532, 0.8007, 0.8824, 0.8201, 0.7667,\n",
      "        0.8325, 0.7654, 0.8201, 0.7824, 0.8443, 0.8523, 0.7780, 0.7448, 0.7763,\n",
      "        0.7377, 0.6849, 0.8719, 0.8288, 0.8497, 0.8356, 0.7474, 0.8634, 0.2268,\n",
      "        0.7491, 0.7083, 0.7760, 0.7117, 0.7918, 0.6601, 0.7908, 0.7184, 0.2888,\n",
      "        0.7677, 0.8005, 0.7626, 0.7850, 0.8738, 0.7818, 0.7709, 0.8526, 0.8576,\n",
      "        0.8500, 0.6365, 0.8522, 0.6601, 0.7488, 0.2975, 0.6800, 0.7334, 0.7895,\n",
      "        0.8532, 0.8371, 0.8002, 0.6407, 0.7231, 0.7409, 0.7807, 0.8158, 0.8030,\n",
      "        0.7470, 0.7510, 0.5425, 0.7456, 0.7747, 0.6928, 0.2944, 0.6655, 0.8017,\n",
      "        0.8320, 0.8543, 0.8706, 0.7490, 0.7002, 0.6721, 0.2078, 0.8188, 0.7823,\n",
      "        0.7622, 0.6928, 0.8551, 0.7606, 0.7567, 0.8218, 0.8284, 0.7424, 0.8015,\n",
      "        0.8228, 0.7928, 0.7859, 0.7522, 0.0859, 0.7630, 0.7647, 0.5456, 0.7390,\n",
      "        0.7445, 0.5827, 0.7947, 0.7373, 0.8003, 0.7733, 0.2957, 0.7636, 0.8016,\n",
      "        0.8388, 0.7932, 0.7061, 0.8098, 0.2505, 0.8461, 0.8098, 0.8361, 0.8259,\n",
      "        0.8411, 0.1325, 0.8156, 0.7403, 0.7458, 0.7288, 0.6791, 0.7915, 0.6145,\n",
      "        0.7914, 0.8779, 0.7721, 0.8181, 0.7276, 0.8194, 0.1516, 0.8336, 0.8378,\n",
      "        0.7347, 0.9317, 0.8010, 0.8412, 0.7457, 0.7246, 0.2098, 0.8024, 0.8727,\n",
      "        0.7811, 0.6902, 0.8584, 0.8504, 0.1366, 0.7179, 0.7260, 0.7524, 0.7520,\n",
      "        0.7598, 0.8569, 0.7457, 0.8377, 0.8563, 0.8060, 0.8476, 0.7624, 0.8385,\n",
      "        0.7280, 0.7858, 0.8011, 0.7312, 0.7557, 0.7977, 0.7969, 0.8166, 0.8021,\n",
      "        0.6976, 0.6706, 0.7621, 0.8224, 0.1831, 0.7642, 0.7640, 0.7003, 0.7912,\n",
      "        0.7399, 0.8708, 0.7408, 0.7345, 0.8529, 0.7039, 0.7920, 0.7502, 0.1899,\n",
      "        0.1041, 0.8139, 0.3663, 0.7292, 0.8421, 0.7799, 0.8777, 0.7520, 0.7325,\n",
      "        0.2458, 0.7220, 0.6516, 0.8404, 0.8132, 0.7653, 0.7561, 0.7441, 0.8359,\n",
      "        0.8345, 0.7686, 0.7412, 0.6681, 0.8497, 0.6728, 0.7272, 0.8016, 0.8316,\n",
      "        0.7760, 0.7243, 0.2786, 0.7240, 0.7294, 0.7202, 0.3829, 0.7830, 0.7707,\n",
      "        0.7600, 0.7652, 0.8157, 0.7276, 0.7202, 0.7549, 0.7212, 0.7563, 0.8582,\n",
      "        0.7640, 0.8510, 0.8005, 0.6367, 0.7305, 0.8009, 0.8029, 0.7939, 0.8288,\n",
      "        0.7583, 0.7710, 0.7578, 0.7684, 0.8767, 0.8334, 0.7093, 0.7717, 0.7601,\n",
      "        0.8194, 0.7206, 0.8028, 0.8436, 0.2132, 0.8029, 0.7320, 0.7605, 0.8205,\n",
      "        0.7186, 0.8638, 0.7442, 0.3013, 0.7588, 0.7771, 0.7539, 0.6480, 0.2554,\n",
      "        0.8160, 0.7496, 0.7694, 0.7949, 0.8096, 0.8174, 0.8084, 0.6491, 0.8146,\n",
      "        0.7221, 0.7849, 0.5801, 0.7922, 0.7394, 0.2555, 0.7655, 0.8112, 0.8402,\n",
      "        0.7376, 0.8210, 0.8095, 0.7888, 0.3207, 0.7938, 0.7490, 0.7549, 0.7898,\n",
      "        0.7921, 0.8186, 0.7336, 0.8276, 0.7920, 0.7364, 0.7007, 0.3114, 0.7771,\n",
      "        0.8236, 0.7715, 0.7361, 0.7277, 0.8477, 0.8211, 0.8228, 0.6200, 0.8466,\n",
      "        0.7333, 0.7984, 0.7368, 0.2651, 0.8359, 0.7306, 0.8008, 0.8766, 0.8250,\n",
      "        0.8694, 0.8172, 0.1956, 0.8328, 0.8311, 0.7665, 0.7613, 0.7750, 0.7524,\n",
      "        0.8148, 0.7356, 0.7233, 0.7276, 0.8007, 0.7779, 0.7451, 0.7815, 0.8293,\n",
      "        0.6922, 0.7830, 0.7247, 0.8446, 0.7455, 0.7880, 0.9051, 0.7991, 0.7721,\n",
      "        0.7235, 0.8439, 0.7791, 0.3232, 0.8118, 0.8268, 0.7821, 0.8112, 0.7882,\n",
      "        0.8316, 0.8050, 0.7507, 0.6424, 0.7401, 0.7723, 0.7365, 0.8553, 0.8052,\n",
      "        0.7473, 0.7266, 0.7518, 0.8425, 0.7490, 0.4735, 0.7326, 0.7749, 0.6550,\n",
      "        0.7598, 0.8196, 0.7962, 0.7603, 0.8455, 0.8304, 0.7779, 0.8690, 0.7906,\n",
      "        0.7657, 0.7232, 0.7646, 0.7920, 0.7906, 0.2222, 0.7321, 0.6409, 0.7818,\n",
      "        0.7223, 0.7561, 0.7349, 0.7863, 0.8207, 0.8210, 0.6678, 0.7929, 0.6849,\n",
      "        0.8224, 0.6908, 0.7975, 0.8961, 0.7682, 0.6965, 0.7924, 0.7515, 0.7973,\n",
      "        0.7852, 0.7308, 0.2778, 0.8134, 0.6690, 0.8699, 0.7585, 0.7961, 0.7383,\n",
      "        0.7595, 0.8590, 0.7970, 0.7991, 0.8468, 0.8895, 0.7789, 0.7550, 0.7185,\n",
      "        0.7894, 0.7549, 0.2577, 0.8245, 0.8336, 0.7608, 0.7221, 0.7086, 0.6825,\n",
      "        0.7257, 0.7313, 0.7841, 0.8112, 0.7718, 0.7955, 0.7077, 0.7788, 0.7295,\n",
      "        0.7965, 0.7551, 0.7385, 0.8320, 0.7559, 0.2502, 0.7917, 0.7681, 0.7621,\n",
      "        0.7663, 0.7629, 0.7130, 0.7854, 0.8311, 0.7599, 0.7456, 0.8403, 0.8088,\n",
      "        0.7871, 0.2660, 0.7285, 0.8482, 0.7103, 0.7069, 0.7980, 0.7394, 0.6892,\n",
      "        0.6851, 0.7844, 0.7499, 0.7444, 0.8040, 0.8043, 0.5894, 0.6714, 0.7921,\n",
      "        0.7899, 0.7573, 0.7816, 0.7674, 0.2630, 0.8033, 0.7589, 0.8377, 0.6753,\n",
      "        0.8244, 0.7487, 0.7649, 0.8117, 0.7807, 0.8462, 0.8042, 0.1437, 0.6919,\n",
      "        0.7313, 0.8561, 0.8156, 0.8102, 0.7798, 0.8044, 0.7746, 0.7044, 0.7959,\n",
      "        0.7243, 0.1237, 0.8263, 0.6973, 0.7964, 0.2443, 0.7769, 0.7487, 0.7347,\n",
      "        0.7825, 0.8071, 0.6846, 0.8261, 0.8203, 0.8496, 0.8232, 0.7217, 0.6763,\n",
      "        0.8029, 0.8302, 0.8343, 0.7410, 0.7378, 0.8219, 0.7120, 0.7794, 0.7909,\n",
      "        0.7970, 0.7891, 0.8326, 0.7259, 0.8081, 0.7738, 0.5392, 0.8033, 0.7986,\n",
      "        0.8253, 0.2257, 0.8043, 0.8157, 0.7496, 0.8365, 0.8051, 0.7322, 0.8151,\n",
      "        0.7499, 0.8111, 0.8003, 0.6811, 0.8384, 0.8045, 0.8132, 0.8746, 0.7578,\n",
      "        0.6777, 0.6958, 0.7867, 0.7432, 0.8259, 0.7645, 0.7766, 0.8594, 0.8198,\n",
      "        0.7210, 0.7654, 0.7148, 0.8812, 0.7008, 0.8131, 0.7299, 0.8183, 0.6835,\n",
      "        0.8086, 0.6624, 0.2636, 0.7853, 0.7371, 0.9013, 0.7996, 0.7393, 0.7189,\n",
      "        0.8034, 0.8459, 0.7760, 0.7821, 0.7549, 0.7916, 0.7639, 0.7268, 0.8172,\n",
      "        0.7345, 0.7278, 0.7416, 0.7774, 0.7924, 0.7941, 0.8186, 0.8396, 0.7104,\n",
      "        0.8351, 0.7564, 0.7192, 0.7926, 0.7035, 0.8210, 0.7127, 0.7601, 0.7884,\n",
      "        0.7593, 0.7395, 0.8733, 0.8337, 0.7219, 0.7469, 0.1225, 0.7224, 0.6651,\n",
      "        0.7723, 0.8223, 0.7785, 0.8131, 0.8559, 0.8188, 0.7936, 0.8501, 0.7702,\n",
      "        0.8069, 0.7817, 0.7879, 0.7429, 0.2991, 0.7761, 0.7191, 0.7038, 0.8204,\n",
      "        0.7845, 0.7679, 0.7749, 0.8638, 0.7579, 0.8167, 0.8554, 0.7357, 0.8161,\n",
      "        0.9011, 0.7792, 0.7295, 0.8480, 0.8364, 0.8145, 0.7901, 0.7947, 0.7210,\n",
      "        0.8773, 0.7544, 0.7750, 0.7122, 0.6910, 0.7477, 0.2338, 0.8290, 0.7895,\n",
      "        0.7559, 0.7395, 0.8262, 0.6973, 0.6339, 0.7452, 0.8102, 0.7933, 0.6548,\n",
      "        0.7687, 0.6668, 0.7920, 0.7347, 0.8432, 0.7073, 0.7292, 0.8372, 0.7189,\n",
      "        0.7149, 0.8043, 0.8596, 0.8502, 0.7932, 0.8357, 0.7641, 0.6555, 0.7721,\n",
      "        0.7829, 0.7854, 0.8262, 0.8612, 0.6990, 0.7270, 0.7458, 0.8460, 0.6910,\n",
      "        0.8758, 0.6756, 0.7351, 0.8205, 0.8621, 0.7800, 0.7316, 0.7755, 0.7001,\n",
      "        0.7393, 0.7687, 0.7912, 0.8803, 0.8465, 0.7670, 0.7903, 0.8335, 0.7099,\n",
      "        0.7163, 0.8329, 0.7931, 0.6378, 0.8097, 0.7553, 0.7360, 0.8020, 0.8034,\n",
      "        0.6588, 0.7901, 0.9008])\n"
     ]
    }
   ],
   "source": [
    "### 학습 후 parameter 변화.. distilbert는 변하지 않았다. \n",
    "print(model.classifier.state_dict()['weight'])\n",
    "print(model.distilbert.state_dict()['embeddings.word_embeddings.weight'])\n",
    "print(model.distilbert.state_dict()['embeddings.LayerNorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1385,  0.1663])\n",
      "tensor(1)\n",
      "tensor([0.4244, 0.5756])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chkim\\AppData\\Local\\Temp\\ipykernel_16728\\3315986766.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prediction = torch.nn.functional.softmax(logits).argmax(dim=-1)\n",
      "C:\\Users\\chkim\\AppData\\Local\\Temp\\ipykernel_16728\\3315986766.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(torch.nn.functional.softmax(logits))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    logits = model(**tokenizer(train.document[8999], return_tensors='pt'))['logits'][0]\n",
    "    prediction = torch.nn.functional.softmax(logits).argmax(dim=-1)\n",
    "    print(logits)\n",
    "    print(prediction)\n",
    "    print(torch.nn.functional.softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 배치별 길이 다르게 padding 하는 방법 2번째 : Dataset, Dataloader 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = pd.read_csv('nsmc/ratings_train.txt', sep = '\\t')\n",
    "        self.train = self.train[~pd.isnull(self.train.document)]\n",
    "        self.text = self.train.document.values.tolist()\n",
    "        self.label = self.train.label.values.tolist()\n",
    "    def __getitem__(self, i):\n",
    "        return self.text[i], self.label[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('아 더빙.. 진짜 짜증나네요 목소리', 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CustomDataset()\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collate_fn을 만들자. \n",
    "### collate_fn은 dataloader 와 dataset 사이에 있다고 생각하면된다. dataset에서 넘어온것들이 collate_fn을 거친다. \n",
    "def make_batch_pad(samples):\n",
    "    inputs = tokenizer([sample[0] for sample in samples], padding=True, add_special_tokens=True, return_tensors='pt')\n",
    "    labels = torch.LongTensor([sample[1] for sample in samples])\n",
    "    \n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 4, collate_fn = make_batch_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 56])\n",
      "tensor([0, 1, 0, 0])\n",
      "torch.Size([4, 101])\n",
      "tensor([1, 0, 0, 0])\n",
      "torch.Size([4, 67])\n",
      "tensor([1, 1, 1, 1])\n",
      "torch.Size([4, 99])\n",
      "tensor([0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "## 배치단위로 padding이 되었다. \n",
    "cnt=0\n",
    "for input, label in dataloader:\n",
    "    print(input['input_ids'].shape)\n",
    "    print(label)\n",
    "    cnt+=1\n",
    "    if cnt==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0420, -0.0604],\n",
       "        [-0.0432, -0.0567],\n",
       "        [ 0.0328, -0.0197],\n",
       "        [-0.0486, -0.0581]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(**input)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7e89925b1aa50b605f20d7e318acc682aa3f2d8d9d7bfa0a81657dbff3df5c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
