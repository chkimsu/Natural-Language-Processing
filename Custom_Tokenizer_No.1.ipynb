{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n",
    "train = train[~pd.isnull(train['document'])]\n",
    "train_text = train['document'].values.tolist()\n",
    "\n",
    "batch_size = 50\n",
    "all_texts = [train_text[i : i + batch_size] for i in range(0, len(train_text), batch_size)]\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(train_text), batch_size):\n",
    "        yield train_text[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_practice\\\\tokenizer_config.json',\n",
       " './tokenizer_practice\\\\special_tokens_map.json',\n",
       " './tokenizer_practice\\\\vocab.txt',\n",
       " './tokenizer_practice\\\\added_tokens.json',\n",
       " './tokenizer_practice\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('./tokenizer_practice')\n",
    "new_tokenizer.save_pretrained('./tokenizer_practice')\n",
    "\n",
    "## 알 수 있듯이 distilbert의 경우는 vocab.txt를 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train('nsmc/ratings_train.txt', vocab_size=150, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=261, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer ## 보면 알 수 있듯이 vocab_size를 150으로 했지만 알고리즘에 의해서 최소 261개가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizer_practice\\\\vocab.json', './tokenizer_practice\\\\merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model('./tokenizer_practice')  ## Bytelevel의 경우에는 vocab.json과 merges.txt가 생긴다. 위에서 vocab.txt와는 상반된다. \n",
    "## 해당되는 pretrained tokenizer와 접합시키기 위해서는 pretrained tokenize가 vocab.json인지 vocab.txt를 쓰는지 확인해야한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 자신만의 tokenizer 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have a dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### normalize pre_tokenize를 각각지정해준다. pretokenize는 tokenize 전단계를 의미한다. 헷갈리지 말자. \n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "tokenizer.normalizer.normalize_str('I HAVE A dOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('is', (5, 7)),\n",
       " ('an', (8, 10)),\n",
       " ('example', (11, 18)),\n",
       " ('OF', (19, 21)),\n",
       " ('THE', (22, 25)),\n",
       " ('WORLD', (26, 31))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example OF THE WORLD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=1200, special_tokens=special_tokens)\n",
    "\n",
    "### 이건 special token이라는걸 지정만 해줄뿐 각각이 뭔지를 나타내는건 아니다. vocab에다가 추가하려고 하는것!!\n",
    "### vocab_size도 최소라는게 있는것같다. 800보다 작게 하면 무조건 800으로 뜨고 1200으로 하면 1200으로 뜬다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'cls_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a1bd315ac4c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls_token_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'cls_token_id'"
     ]
    }
   ],
   "source": [
    "tokenizer.cls_token_id## 아직까지는 cls 토큰이라는게 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", cls_token_id),\n",
    "        (\"[SEP]\", sep_token_id),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tokenizer.encode(\"This is one sentence.\", \"With this one we have a pair.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 't',\n",
       " '##h',\n",
       " '##i',\n",
       " '##s',\n",
       " 'i',\n",
       " '##s',\n",
       " 'o',\n",
       " '##n',\n",
       " '##e',\n",
       " 's',\n",
       " '##e',\n",
       " '##n',\n",
       " '##t',\n",
       " '##e',\n",
       " '##n',\n",
       " '##c',\n",
       " '##e',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'w',\n",
       " '##i',\n",
       " '##t',\n",
       " '##h',\n",
       " 't',\n",
       " '##h',\n",
       " '##i',\n",
       " '##s',\n",
       " 'o',\n",
       " '##n',\n",
       " '##e',\n",
       " 'w',\n",
       " '##e',\n",
       " 'h',\n",
       " '##a',\n",
       " '##v',\n",
       " '##e',\n",
       " 'a',\n",
       " 'p',\n",
       " '##a',\n",
       " '##i',\n",
       " '##r',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 구글에서는 BERT 학습할때 WPM 공개안했지만, HUGGINGFACE에서 공개했다고 함. 이건 HUGGINGFACE 사용하는 방법. 다른사람거. \n",
    "### huggingface로 wordpiece기반의 custom tokenizer만드는것. \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_corpus_path():\n",
    "    corpus_paths = []\n",
    "    for current, dirs, files in os.walk(\"corpus\"):\n",
    "        for file in files:\n",
    "            corpus_paths.append(os.path.join(current, file))\n",
    "    return corpus_paths\n",
    "\n",
    "\n",
    "def WPM(size, corpora):\n",
    "    wpm_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "    wpm_tokenizer.train(corpora, trainer)\n",
    "    wpm_tokenizer.save(\"vocab/WPM_everyone.json\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    corpus_paths = [\"./corpus_pp.txt\"]\n",
    "    WPM(32000, corpus_paths)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a76ec67cc700e8b4b69919c38150e027361854285467e972b878ad1dc6c1f25"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('graph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
