{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model2 = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer2 = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[UNK]', '[SEP]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer('임베딩귉')['input_ids']) ### unkknown도 분명있다 찾기 힘들뿐. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '이순신은 조선 중기의 무신이다.'\n",
    "text2 = '나는 강아지다'\n",
    "text3 = '그의 이름은 최수빈인 것 같기도 하고 아닌것 같기도 하다. '\n",
    "batch_text = [text, text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1463, 30019, 29997, 30014, 30021, 29997, 30019, 30021, 29999, 30017, 30021, 1464, 30011, 29997, 30008, 30021, 1464, 30014, 30025, 29991, 30019, 29999, 30018, 1459, 30014, 29997, 30019, 30021, 29999, 30019, 29993, 30006, 1012, 102]\n",
      "{'input_ids': [101, 1463, 30019, 29997, 30014, 30021, 29997, 30019, 30021, 29999, 30017, 30021, 1464, 30011, 29997, 30008, 30021, 1464, 30014, 30025, 29991, 30019, 29999, 30018, 1459, 30014, 29997, 30019, 30021, 29999, 30019, 29993, 30006, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1463, 30019, 29997, 30014, 30021, 29997, 30019, 30021, 29999, 30017, 30021, 1464, 30011, 29997, 30008, 30021, 1464, 30014, 30025, 29991, 30019, 29999, 30018, 1459, 30014, 29997, 30019, 30021, 29999, 30019, 29993, 30006, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))  ## encode는 그저 input_ids만 출력해주고\n",
    "print(tokenizer.encode_plus(text)) ## encode_plus는 마스킹 등 모두 출력해준다. \n",
    "print(tokenizer(text)) ## 보니까 __call__ 함수는 똑같은 것 같다. encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1463, 30019, 29997, 30014, 30021, 29997, 30019, 30021, 29999,\n",
      "         30017, 30021,  1464, 30011, 29997, 30008, 30021,  1464, 30014, 30025,\n",
      "         29991, 30019, 29999, 30018,  1459, 30014, 29997, 30019, 30021, 29999,\n",
      "         30019, 29993, 30006,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  101,  1463, 30019, 29997, 30014, 30021, 29997, 30019, 30021, 29999,\n",
      "         30017, 30021,  1464, 30011, 29997, 30008, 30021,  1464, 30014, 30025,\n",
      "         29991, 30019, 29999, 30018,  1459, 30014, 29997, 30019, 30021, 29999,\n",
      "         30019, 29993, 30006,  1012,   102],\n",
      "        [  101,  1456, 30006, 29992, 30017, 30021,  1455, 30006, 30025, 29999,\n",
      "         30006, 30000, 30019, 29993, 30006,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "## 2개의 text를 넣을때는 이렇게 padding혹은 truncate을 해주어야 한다. 안 그러면 에러가 남. \n",
    "## 밑에서 각각의 text input계산은 잘 하던데 한번에 배치로 처리하라때는 길이를 맞추어줘야하는듯하다.\n",
    "\n",
    "print(tokenizer(text, return_tensors='pt'))\n",
    "print(tokenizer(batch_text, return_tensors='pt', padding=True)) ## 이렇게 하면 batch 단위로 padding이 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 768])\n",
      "torch.Size([1, 16, 768])\n",
      "torch.Size([2, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "print(model(**tokenizer(text, return_tensors='pt'))['last_hidden_state'].shape)  ## 1, 35,768\n",
    "print(model(**tokenizer(text2, return_tensors='pt'))['last_hidden_state'].shape)  ## 1, 16, 768\n",
    "print(model(**tokenizer(batch_text, return_tensors='pt', padding=True))['last_hidden_state'].shape) ## 2,35, 768\n",
    "\n",
    "## BERT 모델의 경우에는 어떤 입력을 넣어도 입력에 해당하는 길이만큼만 뱉어준다.\n",
    "## 즉, 한개의 입력을 넣을때는 bert모델 학습시킬때 썼던 뭐..최대길이 이런걸 맞춰줄 필요가 없다. 이건 알아보니 self attention이 dot product로 되어있어서\n",
    "## 굳이 안맞추어도 길이가 다른것들도 학습이 가능하고 prediction도 가능하다고 한다. \n",
    "## ** 이거 안해주면 에러난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1456, 30006, 29992, 30017, 30021, 100, 100, 1029, 102, 1456, 30006, 29992, 30017, 30021, 1469, 30011, 29994, 30006, 30025, 29999, 30019, 29993, 30006, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 두 문장을 넣을때는 다음과 같다. \n",
    "tokenizer('나는 최수빈이다 그럴까?', '나는 호랑이다', return_token_type_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1456, 30006, 29992, 30017, 30021, 100, 100, 1029, 102, 1456, 30006, 29992, 30017, 30021, 1469, 30011, 29994, 30006, 30025, 29999, 30019, 29993, 30006, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2('나는 최수빈이다 그럴까?', '나는 호랑이다', return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.1219,  0.1607, -0.9875,  ..., -0.5183,  0.8429, -0.0204],\n",
       "         [-0.6121, -0.3777, -0.7107,  ...,  0.2729,  1.2838, -0.2153],\n",
       "         [-0.8598, -0.2312, -0.2313,  ..., -0.1878,  0.0265, -0.9636],\n",
       "         ...,\n",
       "         [-0.1241, -0.1569, -0.1264,  ..., -0.2564,  0.0171, -0.6824],\n",
       "         [-0.5016, -0.1034, -0.2702,  ..., -0.1109,  0.1309, -1.2801],\n",
       "         [ 0.5553, -0.2568, -0.5277,  ...,  0.0482, -0.3414, -0.3930]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9835, -0.9031, -0.9999,  0.9958,  0.9958, -0.8062,  0.9953,  0.8118,\n",
       "         -0.9998, -1.0000, -0.9912,  0.9998,  0.9511,  0.9948,  0.9662, -0.9948,\n",
       "         -0.9805, -0.9207,  0.8079, -0.9715,  0.9490,  1.0000, -0.9515,  0.8374,\n",
       "          0.9202,  1.0000, -0.9894,  0.9171,  0.9684,  0.6693, -0.9870,  0.8270,\n",
       "         -0.9666, -0.8135, -0.9999, -0.9982,  0.9418, -0.8669, -0.7864, -0.6617,\n",
       "         -0.9328,  0.8663,  1.0000,  0.9493,  0.8855, -0.7871, -1.0000,  0.8965,\n",
       "         -0.9195,  1.0000,  0.9998,  0.9999,  0.8774,  0.9520,  0.9437, -0.9554,\n",
       "          0.7498,  0.7745, -0.8354, -0.9495, -0.8939,  0.8970, -0.9997, -0.9662,\n",
       "          1.0000,  0.9998, -0.9009, -0.8726, -0.8344,  0.7183,  0.9916,  0.7947,\n",
       "         -0.8523, -0.8985,  0.9993,  0.8686, -0.9227,  1.0000, -0.9854, -0.9715,\n",
       "          0.9994,  0.9996,  0.9205, -0.9977,  0.9973, -1.0000,  0.9785, -0.6435,\n",
       "         -0.9631,  0.8515,  0.9532, -0.8226,  0.9949,  0.9534, -0.9812, -0.9578,\n",
       "         -0.9058, -0.9997, -0.9139, -0.9407,  0.8389, -0.8544, -0.9580, -0.8430,\n",
       "          0.9198, -0.9578, -0.9309,  0.9747,  0.9755,  0.9605,  0.8528, -0.9048,\n",
       "          0.9616, -0.9825,  0.9556, -0.8777, -0.9672, -0.9179, -0.9652,  0.9203,\n",
       "         -0.9564, -0.8722,  0.9683, -0.9779,  0.9013, -0.8574, -1.0000, -1.0000,\n",
       "         -0.9942, -0.9796, -0.9521, -0.9180, -0.9475, -0.9386,  0.9388,  0.9279,\n",
       "          0.8384,  1.0000, -0.9138,  0.9492, -0.9859, -0.9973,  0.9979, -0.9462,\n",
       "          0.9972,  0.9655, -0.9717,  0.8255, -0.9652,  0.9631, -0.9882, -0.8348,\n",
       "         -0.9994, -0.8882, -0.8125,  0.9474, -0.9946, -1.0000, -0.9891, -0.8775,\n",
       "         -0.9549,  0.9604,  0.9922,  0.9229, -0.9724,  0.9128,  0.9904,  0.9226,\n",
       "         -0.9407, -0.9588,  0.9284, -0.9102, -0.9998, -0.9596, -0.9087,  0.7952,\n",
       "          0.9772,  0.9261,  0.8301,  0.9988, -0.8529,  0.9867, -0.9605,  0.9526,\n",
       "         -0.8206,  0.7378, -0.9425,  0.9842, -0.9441,  0.9464,  0.9889, -0.9940,\n",
       "         -0.8916, -0.8723, -0.9172, -0.9289, -0.9986,  0.8597, -0.8067, -0.7761,\n",
       "         -0.7930,  0.8911,  0.9999,  0.9140,  0.9927,  0.9565, -0.9821, -0.9317,\n",
       "          0.8260,  0.8682,  0.8824,  0.9873, -0.9798, -0.7485, -0.9534, -0.9736,\n",
       "          0.7867, -0.9536, -0.8527, -0.9264,  0.9920, -0.9333,  0.9980,  0.9202,\n",
       "         -0.9999, -0.9366,  0.9081, -0.8999,  0.9213, -0.7483,  0.4311,  1.0000,\n",
       "         -0.9324,  0.9925,  0.9183, -0.9999, -0.8312,  0.9901, -0.9090,  0.9828,\n",
       "         -0.9774,  0.9993,  0.9999,  0.9962, -0.9457, -0.9984, -0.9837, -0.9994,\n",
       "         -0.8155,  0.9942,  0.9999,  0.9477,  0.9091, -0.9839, -0.9654,  1.0000,\n",
       "         -0.1338, -0.9337,  0.0332, -0.9570, -0.9724,  0.9992,  0.8602,  0.9558,\n",
       "         -0.9326, -0.9660, -0.9233,  0.9964,  0.8526,  0.9999, -0.9225, -0.9984,\n",
       "         -0.9908, -0.9323,  0.7607, -0.8490, -0.9971,  0.6822, -0.9605,  0.9321,\n",
       "          0.8455,  0.9376, -0.9999,  1.0000,  1.0000,  0.9512,  0.9252,  0.9917,\n",
       "         -1.0000, -0.4894,  1.0000, -1.0000, -1.0000, -0.9567, -0.9823,  0.6999,\n",
       "         -1.0000, -0.7930, -0.8194, -0.8932,  0.9990,  0.9382,  1.0000, -1.0000,\n",
       "          0.4624,  0.9458, -0.9607,  0.9999, -0.9458,  0.9436,  0.9752,  0.8667,\n",
       "         -0.8475,  0.9188, -0.9999, -0.9934, -0.9979, -0.9990,  1.0000,  0.8148,\n",
       "         -0.9826, -0.9352,  0.9231, -0.7749,  0.8854, -0.9583, -0.8863,  0.9979,\n",
       "          0.9913,  0.7907,  0.8710, -0.9235,  0.8991,  0.9731,  0.8140,  0.9324,\n",
       "         -0.9531, -0.9306, -0.9809,  0.9117, -0.9985, -0.9504,  0.9784, -0.9317,\n",
       "          0.9999,  1.0000,  0.8050, -0.9697,  0.9864,  0.9127, -0.8581,  1.0000,\n",
       "          0.9949, -0.9225, -0.9296,  0.9791, -0.9703, -0.9833,  0.9999, -0.8366,\n",
       "         -0.9995, -0.9929,  0.9474, -0.9599,  1.0000, -0.9746, -0.9341,  0.9428,\n",
       "          0.9379, -0.9925, -0.5966,  0.9119, -0.9992,  0.8997, -0.9888,  0.9870,\n",
       "          0.9638, -0.7941,  0.9296, -0.9965, -0.9015,  0.8226, -0.9972, -0.9527,\n",
       "          0.9999,  0.9303, -0.8124,  0.7572, -0.9107, -0.0890, -0.9539,  0.9787,\n",
       "          1.0000, -0.9432,  0.9944, -0.9683, -0.6886,  0.8544,  0.9420,  0.9516,\n",
       "         -0.9053, -0.9036,  0.9983, -0.9994, -0.9543,  0.9633,  0.8600, -0.7794,\n",
       "          1.0000,  0.9751,  0.8702,  0.9575,  1.0000,  0.7829,  0.9389,  1.0000,\n",
       "          0.9503, -0.8791,  0.9400,  0.9892, -1.0000, -0.8821, -0.9128,  0.7416,\n",
       "         -0.8595, -0.7492, -0.9614,  0.9574,  1.0000,  0.9331,  0.9067,  0.9965,\n",
       "          1.0000,  0.0166,  0.9680, -0.9878,  0.9963, -1.0000, -0.9815, -0.8432,\n",
       "         -0.8458, -0.9999, -0.9110,  0.8795, -0.9645,  0.9998,  0.9973, -0.9999,\n",
       "         -0.9823, -0.9865,  0.9972,  0.8040, -1.0000, -0.9829, -0.7154,  0.9914,\n",
       "         -0.9385, -0.9381, -0.9937, -0.9000,  0.9522, -0.8196,  0.8926,  0.9999,\n",
       "         -0.9409, -0.9953, -0.9106, -0.7459, -0.9781,  0.9909, -0.9697, -1.0000,\n",
       "         -0.8475,  1.0000, -0.9295,  0.9998,  0.9026,  0.9685, -0.8595,  0.7739,\n",
       "          0.9999,  0.8946, -0.9997, -1.0000, -0.9967, -0.9353,  0.8955,  0.9898,\n",
       "          0.9993,  0.8499,  0.9790,  0.8377, -0.7209,  0.8775,  1.0000, -0.8981,\n",
       "         -0.8230, -0.9204, -0.8302, -0.8858, -0.9729,  1.0000,  0.8557,  0.9668,\n",
       "         -0.9797, -0.9997, -0.9966,  1.0000,  0.8912, -0.8332,  0.9604,  0.9701,\n",
       "         -0.8200,  0.9951, -0.8708, -0.8130,  0.8852,  0.8564,  0.9505, -0.9461,\n",
       "         -0.9354, -0.9205,  0.9510, -0.9327,  1.0000, -0.9598, -0.8672, -0.7830,\n",
       "         -0.9516,  0.9882,  0.6517, -0.9640, -0.8877,  0.8884,  0.9582,  0.9057,\n",
       "         -0.9384, -0.9671,  0.9999,  0.9990, -0.9999, -0.9216,  0.9325, -0.9890,\n",
       "          0.9736,  1.0000,  0.8472,  0.9893,  0.8804, -0.8920,  0.9115, -0.9367,\n",
       "          0.9675, -0.9524, -0.9170, -0.8512,  0.9158, -0.8445, -0.8081,  0.7898,\n",
       "          0.8124, -0.9210, -0.9534, -0.9023,  0.9256,  0.9940, -0.8625, -0.7944,\n",
       "          0.7738, -0.7869, -0.9862, -0.8900, -0.9450, -1.0000,  0.9411, -1.0000,\n",
       "          0.9916,  0.9905, -0.8317,  0.8459,  0.2539,  0.9894, -0.9195, -0.9999,\n",
       "         -0.9753,  0.9005, -0.9456, -0.9953, -0.9227,  0.9345, -0.8506,  0.7546,\n",
       "         -0.9958,  0.8961, -0.8572,  1.0000,  0.8629, -0.9836, -0.9998,  0.8279,\n",
       "         -0.9160,  1.0000, -0.9963, -0.9393,  0.9227, -0.9932, -0.9548,  0.9047,\n",
       "          0.7407, -0.9841, -1.0000,  0.9664,  0.9980, -0.8491,  0.9616, -0.9114,\n",
       "         -0.9497,  0.7891,  0.9998,  0.9701,  0.9266,  0.9909, -0.6114, -0.9264,\n",
       "          0.9383,  0.8341,  0.9552,  0.8361,  1.0000,  0.9028, -0.9574, -0.9329,\n",
       "         -0.9855, -0.9027, -0.9847,  0.8817,  0.8849,  0.9605, -0.8782,  0.9816,\n",
       "         -0.9996,  0.7812, -0.9954, -0.9988,  0.8841, -0.9399, -0.9464, -0.9576,\n",
       "          0.9813, -0.8514, -0.7900,  0.7903,  0.8213,  0.9551,  0.9336, -1.0000,\n",
       "          0.9379,  0.9298,  0.9999,  0.9222,  0.9939,  0.9298,  0.8740, -0.9704,\n",
       "         -0.9998, -0.9140, -0.7943,  0.9781,  0.9570,  0.9255,  0.9139, -0.8736,\n",
       "         -0.6789, -0.9983, -0.1775, -0.9743,  0.9143, -0.9987, -0.9994,  0.9556,\n",
       "          0.8952, -0.7873, -0.9796, -0.9987,  0.9985,  0.9175,  0.9534,  0.7270,\n",
       "          0.8099,  0.9128,  0.9941,  0.9657, -0.9997,  0.9813, -0.9988,  0.8682,\n",
       "          0.1936, -0.9401,  0.8732,  0.9669, -0.9693,  0.8978, -0.8605, -0.9987,\n",
       "          0.4510, -0.8599,  0.9412, -0.9106, -0.8168, -0.9220, -0.7837, -0.7994,\n",
       "         -0.9913,  0.9117,  0.9622,  0.9152,  0.9979, -0.8229, -0.9881, -0.8710,\n",
       "         -0.9996, -0.9291,  0.9984, -0.8664, -0.9972,  0.9974,  0.8354,  0.3035,\n",
       "          0.9552, -0.9012, -0.9137, -0.9620,  0.9640, -0.6741, -0.9581, -0.9743,\n",
       "          0.8797,  0.8713,  1.0000, -0.9993, -0.9999, -0.8550, -0.9052,  0.8630,\n",
       "         -0.9462, -1.0000,  0.9163, -0.9954,  0.9957, -0.9976,  0.9994, -0.9982,\n",
       "         -0.9982, -0.8637,  0.8383,  0.9991, -0.9008, -0.9940,  0.9157, -0.9916,\n",
       "          1.0000,  0.9300, -0.9841, -0.9523,  0.9538, -0.9987, -0.9306,  0.9698]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(**tokenizer2('나는 최수빈이다 그럴까?', '나는 호랑이다', return_token_type_ids=True, return_tensors='pt'))\n",
    "### token_type_ids를 True, False했을때 결과가 다르다. token_type_ids를 False로 하면 다 0으로 채워서 주는듯하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2  ## bert model 구조 잘 살펴보면 word 임베딩, position 임베딩, token type 임베딩 3개가 있다. , 2,768이라는건 0과 1 을 의미하는 거일테고.. \n",
    "model ## token type 임베딩 자체가 distilber는 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a49682947cba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'나는 최수빈이다 그럴까?'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'나는 호랑이다'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m## 이렇게하면 에러가 뜨는게 당연 , DISTILBERT는 TOKEN_TYPE_IDS가 없다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "model(**tokenizer('나는 최수빈이다 그럴까?', '나는 호랑이다', return_token_type_ids=True, return_tensors='pt'))\n",
    "## 이렇게하면 에러가 뜨는게 당연 , DISTILBERT는 TOKEN_TYPE_IDS가 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "## 이건 label조절 불가 무조건 2개 인듯하다. \n",
    "## 어쩌면 학습된 모델에 classification만 붙인것일수도 있겠다.. classifier의 경우는 random하게.. weight초기화된 걸 붙인것인듯하다. 찾아보니 그렇게 나온다. \n",
    "## 단톡방에 물어보니까 fine-tuning한것도 제공한다고 한다. \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0243,  0.1185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(**tokenizer(text, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distilbert\n",
    "model.classifier \n",
    "## 이렇게 두개로 나눠져있어서 freeze할 수 있다. \n",
    "\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "## model에서 distilbert부분은 freeze하는것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0315, -0.0085, -0.0348,  ...,  0.0207,  0.0239,  0.0078],\n",
      "        [ 0.0098, -0.0099,  0.0116,  ..., -0.0058, -0.0259,  0.0040]])\n",
      "tensor([0., 0.])\n",
      "tensor([[-0.0166, -0.0666, -0.0163,  ..., -0.0200, -0.0514, -0.0264],\n",
      "        [-0.0132, -0.0673, -0.0161,  ..., -0.0227, -0.0554, -0.0260],\n",
      "        [-0.0176, -0.0709, -0.0144,  ..., -0.0246, -0.0596, -0.0232],\n",
      "        ...,\n",
      "        [-0.0231, -0.0588, -0.0105,  ..., -0.0195, -0.0262, -0.0212],\n",
      "        [-0.0490, -0.0561, -0.0047,  ..., -0.0107, -0.0180, -0.0219],\n",
      "        [-0.0065, -0.0915, -0.0025,  ..., -0.0151, -0.0504,  0.0460]])\n",
      "tensor([[-0.0024,  0.0224, -0.0207,  ...,  0.0200,  0.0560,  0.0463],\n",
      "        [ 0.0080,  0.0269, -0.0575,  ..., -0.0221,  0.1373,  0.0125],\n",
      "        [-0.0043,  0.0318,  0.0189,  ..., -0.0107,  0.0083, -0.0422],\n",
      "        ...,\n",
      "        [ 0.0120, -0.0023,  0.0710,  ...,  0.0636,  0.0457, -0.0321],\n",
      "        [ 0.0061,  0.0861,  0.0829,  ..., -0.0218,  0.1167,  0.0146],\n",
      "        [-0.0096, -0.0904,  0.0143,  ..., -0.0473, -0.0973, -0.0282]])\n"
     ]
    }
   ],
   "source": [
    "## 학습전. \n",
    "print(model.classifier.state_dict()['weight'])\n",
    "print(model.classifier.state_dict()['bias'])\n",
    "\n",
    "model.distilbert.state_dict().keys() ## 모든 layer 이름.\n",
    "## pytorch에서는 state_dict로 모델 layer이름 가중치 bias에 모두 접근할 수 있다. \n",
    "\n",
    "print(model.distilbert.state_dict()['embeddings.word_embeddings.weight'])\n",
    "print(model.distilbert.state_dict()['transformer.layer.0.attention.q_lin.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.train()\n",
    "\n",
    "for i in range(100):\n",
    "    logits = model(**tokenizer(text, return_tensors='pt'))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    loss = torch.nn.functional.nll_loss(logits['logits'], torch.LongTensor([1]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0315, -0.0085, -0.0348,  ...,  0.0207,  0.0239,  0.0078],\n",
      "        [ 1.0098,  0.9801,  0.7216,  ...,  0.9842,  0.9741,  1.0040]])\n",
      "tensor([0.0000, 1.0000])\n",
      "tensor([[-0.0166, -0.0666, -0.0163,  ..., -0.0200, -0.0514, -0.0264],\n",
      "        [-0.0132, -0.0673, -0.0161,  ..., -0.0227, -0.0554, -0.0260],\n",
      "        [-0.0176, -0.0709, -0.0144,  ..., -0.0246, -0.0596, -0.0232],\n",
      "        ...,\n",
      "        [-0.0231, -0.0588, -0.0105,  ..., -0.0195, -0.0262, -0.0212],\n",
      "        [-0.0490, -0.0561, -0.0047,  ..., -0.0107, -0.0180, -0.0219],\n",
      "        [-0.0065, -0.0915, -0.0025,  ..., -0.0151, -0.0504,  0.0460]])\n",
      "tensor([[-0.0024,  0.0224, -0.0207,  ...,  0.0200,  0.0560,  0.0463],\n",
      "        [ 0.0080,  0.0269, -0.0575,  ..., -0.0221,  0.1373,  0.0125],\n",
      "        [-0.0043,  0.0318,  0.0189,  ..., -0.0107,  0.0083, -0.0422],\n",
      "        ...,\n",
      "        [ 0.0120, -0.0023,  0.0710,  ...,  0.0636,  0.0457, -0.0321],\n",
      "        [ 0.0061,  0.0861,  0.0829,  ..., -0.0218,  0.1167,  0.0146],\n",
      "        [-0.0096, -0.0904,  0.0143,  ..., -0.0473, -0.0973, -0.0282]])\n"
     ]
    }
   ],
   "source": [
    "## 학습후\n",
    "model.eval()\n",
    "print(model.classifier.state_dict()['weight'])\n",
    "print(model.classifier.state_dict()['bias'])\n",
    "\n",
    "model.distilbert.state_dict().keys() ## 모든 layer 이름.\n",
    "## pytorch에서는 state_dict로 모델 layer이름 가중치 bias에 모두 접근할 수 있다. \n",
    "\n",
    "print(model.distilbert.state_dict()['embeddings.word_embeddings.weight'])\n",
    "print(model.distilbert.state_dict()['transformer.layer.0.attention.q_lin.weight'])\n",
    "\n",
    "\n",
    "### 학습하니 바뀌는 것을 확인할 수 있따. 하지만 weight freeze한 부분에서는 안 변하는 것도 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chkim\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(model(**tokenizer(text, return_tensors='pt'))['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chkim\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4890, 0.5110])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.Tensor([0.0435, 0.0877]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a3adc81eaa78eb9c4c47afed92821ed50fbe14b18b994f3b473860487117bdf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('csb': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
